\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{float}
% added this to stop hbadness warning
\hbadness = 5000000

\geometry
{
	headheight = 4ex,
	includehead,
	includefoot,
	paper = a4paper,
	inner = 2.5cm,
	outer = 2.5cm,
	bindingoffset = 0.5cm,
	top = 2cm,
	bottom = 1.5cm
}


\title{\Huge Problem Set 2} 
\vspace{1cm}
\author {\Large Chara Tsirka 03315, Prodromos Avramidis 03291}

\begin{document}

\maketitle
\begin{center}
\vspace{1cm}
\includegraphics[width=0.3\textwidth]{uthlogo.png}
\vspace{2cm}
\end{center}
\begin{center}
  \Huge Neurofuzzy Computing \vspace{1cm}

  \Large Fall Semester 2023-2024 \vspace{1cm}

  \Large Professor: Dimitrios Katsaros
\end{center}


%Problem 1
\newpage
\noindent \textbf{Problem 1}

\noindent Find the minimum of the two-dimensional function: $F(w) = w_1^2+w_2^2+(0.5w_1+w_2)^2+(0.5w_1+w_2)^4$


\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{pr1_a.png}
  
\end{figure}

\noindent with the Conjugate Gradient (Fletcher-Reeves), with initial guess $w(0)= [3, 3]^T$. Perform 
five iterations (if not finding the minimum earlier). Observe and comment on the 
slow/fast convergence towards the minimum $w=(0, 0)^T$. Then, apply the Gradient 
Descent method (accuracy: three decimal points) on the same function and unit step
movement. Perform ten iterations (if not finding the minimum earlier). For each method 
show your analytic calculations. \\ \\ \\


\noindent \underline{\textbf{\textit{Solution:}}}

\noindent In order to find the minimum of the given function with the \textbf{Conjugate Gradient(Fletcher-Reeves)} we first need to calculate \( \nabla f \). \\ \\
\( \nabla f \) = $ \begin{bmatrix}
  \frac{\partial f}{\partial w_1} \\
  \frac{\partial f}{\partial w_2} 
\end{bmatrix} = \begin{bmatrix}
  2w_1 + 2*(0.5w_1+w_2)*0.5 +4*(0.5w_1+w_2)^3 * 0.5 \\
  2w_2 +2*(0.5w_1+w_2) +4*(0.5w_1+w_2)^3 
\end{bmatrix} \hspace*{1cm}(1)$ \\ \\

\vspace{0.5cm}
\noindent In every step we need to calculate:
\begin{itemize}
  \item \( \nabla f_k \) using $w_k$ and $(1)$
  \item $s_k$ = -\( \nabla f_k \)
  \item $\lambda_k = \frac{(\nabla f_k)^T (\nabla f_k)}{s_k^T H s_k}$
  \item $x_{k+1} = x_k + \lambda_k*s_k$
  \item $s_{k+1} = -(\nabla f_{k+1}) + b_k \cdot s_k$, where $b_k = \frac{(\nabla f_{k+1})^T (\nabla f_{k+1})}{(\nabla f_{k})^T (\nabla f_{k})}$
  
\end{itemize}
\vspace{1cm}
\noindent First iteration:\\\\ $w_0 = [3,3]^T$, \( \nabla f_0 \) = $ \begin{bmatrix} 
  192.75 \\
  379.5 
\end{bmatrix}$, $ s_0 = \begin{bmatrix} 
  -192.75 \\
  -379.5 
\end{bmatrix}$, $\lambda_0 \approx 0.003$\\ \\ \\ So, $w_1 \approx [2.375, 1.769]^T$ and $s_1 \approx [-76.655 -146.81]^T$
\newpage

\noindent Second iteration:\\\\ $w_1 = \approx [2.375, 1.769]^T$, \( \nabla f_1 \) = $ \begin{bmatrix} 
  76.655 \\
  146.81 
\end{bmatrix}$, $ s_1 = \begin{bmatrix} 
  -76.655 \\
  -146.81
\end{bmatrix}$, $\lambda_1 \approx 0.007$\\ \\ \\ So, $w_2 \approx [1.809, 0.685]^T$ and $s_2 \approx [-16.085, -26.064]^T$ \\ \\

\noindent Third iteration:\\\\ $w_2 = [1.809, 0.685]^T$, \( \nabla f_2 \) = $ \begin{bmatrix} 
  16.085\\
  26.064 
\end{bmatrix}$, $ s_2 = \begin{bmatrix} 
  -16.085\\
  -26.064
\end{bmatrix}$, $\lambda_2 \approx 0.0238$\\ \\ \\ So, $w_3 \approx [1.426, 0.064]^T$ and $s_3 \approx [-5.465, -5.016]^T$ \\ \\

\noindent Fourth iteration:\\\\ $w_3 = \approx [1.426, 0.064]^T$, \( \nabla f_3 \) = $ \begin{bmatrix} 
  5.465\\
  5.016 
\end{bmatrix}$, $ s_3 = \begin{bmatrix} 
  -5.465\\
  -5.016
\end{bmatrix}$, $\lambda_3 \approx 0.088$\\ \\ \\ So, $w_4 \approx [0.947, -0.375]^T$ and $s_4 \approx [-16.085, -26.064]^T$ \\ \\

\noindent Fifth iteration:\\\\ $w_4 = \approx [0.947, -0.375]^T$, \( \nabla f_4 \) = $ \begin{bmatrix} 
  16.085\\
  26.064 
\end{bmatrix}$, $ s_4 = \begin{bmatrix} 
  -16.085\\
  -26.064
\end{bmatrix}$, $\lambda_4 \approx 0.476$\\ \\ \\ So, $w_5 \approx [-0.335, -0.418]^T$ and $s_5 \approx [-5.052, 2.586]^T$ \\ \\ \\ \\


\noindent We will now use the \textbf{gradient descent method:} \\
As we previously calculated the gradient is\\ \\
\( \nabla f \) = $ \begin{bmatrix}
  \frac{\partial f}{\partial w_1} \\
  \frac{\partial f}{\partial w_2} 
\end{bmatrix} = \begin{bmatrix}
  2w_1 + 2*(0.5w_1+w_2)*0.5 +4*(0.5w_1+w_2)^3 * 0.5 \\
  2w_2 +2*(0.5w_1+w_2) +4*(0.5w_1+w_2)^3 
\end{bmatrix} \hspace*{1cm}(1)$ \\ \\

\noindent For each iteration we need to take the following steps:
\begin{itemize}
  \item Find the gradient $\nabla f(w)$ for the current w 
  \item Update the weights accordingly w = $\lambda * \nabla f(w)$, where $\lambda= 0.01$ \\ \\
\end{itemize}


\noindent First iteration: \\ \\
Starting w0 = [3,3] \\ \\
$\nabla f(w0)= [192.75 , 379.5]$ \\ \\
w1 = [1.073, -0.795], F(w1) = 1.854 \\ \\

\noindent Second iteration: \\ \\
$\nabla f(w1)= [1.852,-2.177]$ \\ \\
w2 = [1.054, -0.773], F(w2) = 1.773 \\ \\

\noindent Third iteration: \\ \\
$\nabla f(w2)= [1.832,-2.099]$ \\ \\
w3 = [1.036, -0.752], F(w3) = 1.696 \\ \\

\noindent Fourth iteration: \\ \\
$\nabla f(w3)= [1.811,-2.025]$ \\ \\
w4 = [1.018, -0.732], F(w4) = 1.624 \\ \\

\noindent Fifth iteration: \\ \\
$\nabla f(w4)= [1.790,-1.955]$ \\ \\
w5 = [0.100, -0.712], F(w5) = 1.554 \\ \\

\noindent Sixth iteration: \\ \\
$\nabla f(w5)= [1.767,-1.889]$ \\ \\
w6 = [0.982, -0.694], F(w6) = 1.488 \\ \\

\noindent Seventh iteration: \\ \\
$\nabla f(w6)= [1.745,-1.826]$ \\ \\
w7 = [0.965, -0.675], F(w7) = 1.425 \\ \\

\noindent Eighth iteration: \\ \\
$\nabla f(w7)= [1.722,-1.765]$ \\ \\
w8 = [0.947, -0.658], F(w8) = 1.365 \\ \\

\noindent Ninth iteration: \\ \\
$\nabla f(w8)= [1.698,-1.708]$ \\ \\
w9 = [0.930, -0.641], F(w9) = 1.308 \\ \\

\noindent Tenth iteration: \\ \\
$\nabla f(w9)= [1.674,-1.654]$ \\ \\
w10 = [0.914, -0.624], F(w10) = 1.253 \\ \\







%Problem 2
\vspace {2cm}
\noindent \textbf{Problem 2}

\noindent Find the minimum of the previous function using the Newton method and fixed step $ \lambda_k = 1$ (even though this is not a quadratic form). \\

\noindent \underline{\textbf{\textit{Solution:}}} \\ 

\noindent The attached code converges after 9 iterations with the min point being (0,0) = 0.



%Problem 3
\newpage
\noindent \textbf{Problem 3}

\noindent For the neural network shown below

\begin{figure}[h]
  \centering
  \includegraphics[width=0.4\textwidth]{pr3_a.png}
  
\end{figure}

\noindent the initial weights and biases are chosen to be $w^1(0) = -3, b^1(0) = 2, w^2(0) = -1, b^2(0) = -1$. An input/target pair is given to be ${\left \{p = 1, t=0 \right \} }$, and the parameter of LReLU is equal to 0.001. Perform two iterations of backpropagation with learning rate a=1.  \\ \\ \\


\noindent \underline{\textbf{\textit{Solution:}}}

\noindent The neural network shown above has two layers. The first layer's transfer function is Swish, so:
\[ F_1 = \frac{x}{1+e^{-x}}, (b=1) \]  
\[ F_1' = \frac{1+e^{-x} + xe^{-x}}{(1+e^{-x})^2}. \]
\\ \\The second layer's transfer function is LReLU with parameter equal to 0.001, so:
\[ F_2 = \begin{cases}
  0.001x & \text{for } x < 0 \\
  x & \text{for } x \geq 0 \\
\end{cases} \]
 
\[ F_2' = \begin{cases}
  0.001 & \text{for } x < 0 \\
  1 & \text{for } x \geq 0 \\
\end{cases}. \]\\

\noindent \textbf{\textit{First iteration of backpropagation:}}
\noindent \\ \\ \\ \\The output of the first layer is: \\ \\$n^1 = w^1p + b^1 = (-3)*1 + 2 \Rightarrow \bm{n^1 = -1}$
\\ \\$a^1 = F_1(n^1) = \frac{-1}{1+e} \Rightarrow \bm{a^1 \approx -0.2689}$

\noindent \\ \\The output of the second layer is: \\ \\$n^2 = w^2a^1 + b^2 = (-1)(-0.2689) + (-1) \Rightarrow \bm{n^2 \approx -0.731}$
\\ \\$a^2 = F_2(n^2) = 0.001(-0.731) \Rightarrow \bm{a^2 \approx -7.31 * 10^{-4}}$
\\ \\ \\The error is: \\$e = t - a^2 = 0 - (-7.31 * 10^{-4}) \Rightarrow \bm{e \approx 7.31 * 10^{-4}}$ \\ \\
 
\noindent We can now perform the backpropagation. The starting point is found at the second layer: \\ \\
$s^2 = -2F_2'(n^2)(t-a) = -2 * 0.001 * 7.31 * 10^{-4} \Rightarrow \bm{s^2 \approx -1.462 * 10^{-6}}$
\\\\ The first layer sensitivity is then computed by backpropagating the sensitivity from the second layer: \\ \\ $s^1 = F_1'(n^1)(w^2)^Ts^2 = \frac{1+e+(-1)e}{(1+e)^2} * (-1) *(-1.462 * 10^{-6}) \Rightarrow \bm{s^1 \approx 1.057 * 10^{-7}}$
\\\\ \\Finally, weight update takes place as follows: \\\\
$w^2(1) = w^2(0) - as^2(a^1)^T = -1 -1 * (-1.462 * 10 ^{-6}) * (-0.2689) \Rightarrow \bm{w^2(1) \approx -1.000000393}$
\\ \\ $b^2(1) = b^2(0) -as^2 = -1 -1 * (-1.462 * 10 ^ {-6}) \Rightarrow \bm{b^2(1) \approx -0.999}$
$\\ \\ \\ w^1(1) = w^1(0) -as^1(p)^T = -3 -1 * (1.057 * 10^{-7}) * 1 \Rightarrow \bm{w^1(1) \approx -3.000000106} $ 
\\ \\$b^1(1) = b^1(0) - as^1 = 2 - 1 * 1.057 * 10^{-7} \Rightarrow \bm{b^1(1) \approx 1.999}$\\ \\ \\

\noindent \textbf{\textit{Second iteration of backpropagation:}}
\noindent \\ \\The output of the first layer is: \\ \\$n^1(2) = w^1(1)p + b^1(1) = -3.000000106 * 1 + 1.999\Rightarrow \bm{n^1(2) \approx -1.001}$
\\ \\$a^1(2) = F_1(n^1(2)) = \frac{-1.001}{1+e^{1.001}} \Rightarrow \bm{a^1 \approx -0.2690}$

\noindent \\ \\The output of the second layer is: \\ \\$n^2(2) = w^2(1)a^1(2) + b^2(1) = (-1.000000393)(-0.2690) + (-0.999) \Rightarrow \bm{n^2 \approx -0.729}$
\\ \\$a^2(2) = F_2(n^2(2)) = 0.001(-0.729) \Rightarrow \bm{a^2 \approx -7.29 * 10^{-4}}$
\\ \\ \\The error is: \\$e(2) = t - a^2(2) = 0 - (-7.29 * 10^{-4}) \Rightarrow \bm{e \approx 7.29 * 10^{-4}}$ \\\\

\newpage
\noindent We can now perform the backpropagation. The starting point is found at the second layer: \\ \\
$s^2(2) = -2F_2'(n^2(2))(t-a) = -2 * 0.001 * 7.29 * 10^{-4} \Rightarrow \bm{s^2 \approx -1.458 * 10^{-6}}$
\\\\ The first layer sensitivity is then computed by backpropagating the sensitivity from the second layer: \\ \\ $s^1(2) = F_1'(n^1(2))(w^2(1))^Ts^2(2) = \frac{1+e^{-1.001}+(-1.001)e^{-1.001}}{(1+e^{-1.001})^2} * (-1.000000393) *(-1.458 * 10^{-6}) \\\\\Rightarrow \bm{s^1(2) \approx 7.79 * 10^{-7}}$
\\\\ \\Finally, weight update takes place as follows: \\\\
$w^2(2) = w^2(1) - as^2(2)(a^1(2))^T = -1.000000393 -1 * (-1.458 * 10 ^{-6}) * (-0.269) \newline \\\Rightarrow \bm{w^2(1) \approx -1.000000785}$
\\ \\ $b^2(2) = b^2(1) -as^2(2) = -0.999 -1 * (-1.458 * 10 ^ {-6}) \Rightarrow \bm{b^2(1) \approx -0.998}$
$\\ \\ \\ w^1(2) = w^1(1) -as^1(2)(p)^T = -3.000000106 -1 * (7.79 * 10^{-7}) * 1 \Rightarrow \bm{w^1(1) \approx -3.000000885} $ 
\\ \\$b^1(2) = b^1(1) - as^1(2) = 1.999 - 1 * 7.79 * 10^{-7} \Rightarrow \bm{b^1(1) \approx 1.998}$\\ \\ \\

% \\\\ \\Finally, weight update takes place as follows: \\\\
% $w^2(2) = w^2(1) - as^2(a^1)^T = -1 -1 * (-1.462 * 10 ^{-8}) * (-0.2689) \Rightarrow \bm{w^2(1) \approx -1.000000004}$
% \\ \\ $b^2(1) = b^2(0) -as^2 = -1 -1 * (-1.462 * 10 ^ {-8}) \Rightarrow \bm{b^2(1) \approx -0.999}$
% $\\ \\ \\ w^1(1) = w^1(0) -as^1(p)^T = -3 -1 * (1.057 * 10^{-9}) * 1 \Rightarrow \bm{w^1(1) \approx -3.000000001} $ 
% \\ \\$b^1(1) = b^1(0) - as^1 = 2 - 1 * 1.057 * 10^{-9} \Rightarrow \bm{b^1(1) \approx 1.999}$\\ \\ \\

%Problem 4
\newpage
\noindent \textbf{Problem 4} \\

\noindent Write a (MATLAB/python/Keras/…) program to implement the backpropagation
algorithm for a $1-S^1-1$ network (logsig-ReLU). Write the program using matrix
operations, as we did in the class lecture. Choose the initial weights and biases to be
random numbers uniformly distributed between -0.5 and 0.5, and train the network to
approximate the function:
\begin{center}
  $ g(p)=1+sin[p(3 \pi /8)] for -2 \leq p \leq 2.$
\end{center}

\noindent Use $S^1=2, S^1=8, s^1=12$  Experiment with several different values for the learning
rate a (make sure you experiment with a = 0.1), and use several different initial conditions.
Discuss the convergence properties and the accuracy of the algorithm as the learning rate
changes, and as the capacity (in terms of number of hidden neurons) increases. \\ \\ \\

\noindent \underline{\textbf{\textit{Solution:}}} \\ 

\noindent We experimented with 3 different values for the learning rate a:
\begin{enumerate}
  \item For a = 0.01: \\
  \includegraphics[width=0.3\textwidth]{Problem4_2_0.01.png}
  \includegraphics[width=0.3\textwidth]{Problem4_8_0.01.png}
  \includegraphics[width=0.3\textwidth]{Problem4_12_0.01.png}
  \item For a = 0.1: \\
  \includegraphics[width=0.3\textwidth]{Problem4_2_0.1.png}
  \includegraphics[width=0.3\textwidth]{Problem4_8_0.1.png}
  \includegraphics[width=0.3\textwidth]{Problem4_12_0.1.png}
  \item For a = 0.2: \\
  \includegraphics[width=0.3\textwidth]{Problem4_2_0.2.png}
  \includegraphics[width=0.3\textwidth]{Problem4_8_0.2.png}
  \includegraphics[width=0.3\textwidth]{Problem4_12_0.2.png}
\end{enumerate}

\noindent From the graphs we observe that:
\begin{enumerate}
  \item The more neurons a network has the more accurate it is.
  \item 
  As the number of neurons increases, the incremental improvement in accuracy becomes less noticeable, indicating diminishing returns.
  \item The graph becomes inaccurate when the learning rate is either too small or too large.
\end{enumerate}

%Problem 5
\newpage
\noindent \textbf{Problem 5} \\

\noindent In the setting with $S^1= 12$ and learning rate a=0.1 of the previous exercise, apply the
dropout technique (https://jmlr.org/papers/v15/srivastava14a.html) with dropout
probability $\theta$ of hidden-layer neurons equal to $\theta=0.15$, then with $\theta=0.25$, and then with
$\theta=0.35$. Apply the dropout during the training phase only, and only on the hidden layer
units (not of course to the input neuron). Discuss the convergence properties of the
algorithm, as well as its accuracy, and contrast your findings with those of the previous
exercise. Perform any additional experiments to figure out the operation of dropout as a
generalization technique. \\ \\ \\

\noindent \underline{\textbf{\textit{Solution:}}} 

\noindent For each of the $\theta$ values we get the following graphs:\\
\begin{enumerate}
  \item For $\theta=0.15:$ 
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.5\textwidth]{Problem5_0.15.png}
        \end{figure}  
  \item For $\theta=0.25:$ 
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.5\textwidth]{Problem5_0.25.png}
        \end{figure} 
  \newpage
  \item For $\theta=0.35:$ 
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.5\textwidth]{Problem5_0.35.png}
        \end{figure}  
\end{enumerate} 
\noindent \\While the algorithm may not be as accurate as in Problem 4, we have successfully achieved our goal of preventing overfitting.

%Problem 6
\vspace{2cm}
\noindent \textbf{Problem 6}

\noindent Consider the network shown in the figure below, where the inputs to the neuron involve 
both the original inputs and their product \\
\begin{figure}[h]
  \centering
  \includegraphics[width=0.3\textwidth]{pr6.png}
\end{figure}
\newline
\noindent A. Find a learning rule for the network parameters, using the steepest descent algorithm 
(as we have done in the class for backpropagation).\\\\
B. For the following initial parameter values, inputs and target, perform one iteration of 
your learning rule with learning rate $\alpha$ = 1:$w_1= 1, w_2= -1, w_{1,2}= 0.5, b_1 = 1, and p_1= 0, p_2= 1, t= 0.75$. \\ \\


\noindent \underline{\textbf{\textit{Solution:}}}
\noindent Code attached.  

\noindent After 1 iteration w1: 1, w2: -0.25, $w_1,2$: 0.5, b1: 1.75, Error E: 0.28125, Output: 0



%Problem 7
\newpage
\noindent \textbf{Problem 7}

\noindent Show that an MLP using only ReLU (or pReLU) constructs a continuous piecewise linear function. \\ \\

\noindent \underline{\textbf{\textit{Solution:}}}  

\noindent The ReLU function is defined as f(x) = max(0,x).\\
The pReLU function is defined as f(x) = max(ax,x) with a being a small coefficient.\\
In an MLP, each neuron computes a weighted sum of its inputs and applies an activation function to the sum. For a neuron 
with input x weight w and bias b the output before activation is w * x + b and after activation with ReLU it is max(0, w*x + b) while after pReLU it is max(a(w*x+b), w*x+b). \\

\noindent$\bm{Continuity:} $\\
Both the ReLU and pReLU functions are continuous functions. The MLP applies a continuous function for each layer so the overall function is also continuous.\\\\

\noindent$\bm{Piecewise}$ $ \bm{Linearity:} $\\
The ReLU function is piecewise linear, being linear (with a slope of 1) for positive inputs and zero elsewhere. \\\\
The pReLU function is also piecewise linear but with two linear pieces, one with a positive slope (usually 1) and another with a smaller positive slope (the parameter a). \\\\
In an MLP, each neuron applies a linear transformation (weighted sum) followed by a piecewise linear activation (ReLU/pReLU). This makes the output of each neuron piecewise linear. \\\\
The composition of piecewise linear functions remains piecewise linear. Thus, the entire network, being a composition of linear transformations and piecewise linear activations, represents a piecewise linear function.

\noindent \\In conclusion an MLP using only ReLU or pReLU constructs a continuous piecewise linear function.


%Problem 8
\newpage
\noindent \textbf{Problem 8} \\
Consider the following function:
\begin{center}
    $ F(w) = 0.1w_1^2 + 2w_2^2$
\end{center}

\begin{enumerate}
    \item Find the minimum of the function implementing the Adadelta optimizer (instead of
            gradient descent) with learning rate $\alpha = 0.4$. Plot the algorithm’s trajectory on a contour
            plot of F(x)
    \item Change the learning rate to $\alpha = 3$, and repeat the same task.
    \item Try out Adadelta for the same objective function rotated by 45degrees, i.e., 
            $ F(w) = 0.1(w_1+w_2)^2 + 2(w_1-w_2)^2$. Does it behave differently?\\ 
\end{enumerate} 

\noindent \underline{\textbf{\textit{Solution:}}} 

\begin{enumerate}
    \item For a = 0.4 converged after 870 iterations:
          \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{problem80.4.png}      
          \end{figure}
    \item For a = 3 converged after 223 iterations:
          \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{problem83.png}            
          \end{figure} 
          \newpage
    \item Adadelta, fucntion rotated by 45 degrees with a = 3 converged after 208 iterations:
          \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{problem8rotated.png}            
          \end{figure} 
          \noindent \\It does not behave differently. 
            
\end{enumerate}



%Problem 9
\newpage
\noindent \textbf{Problem 9} \\

\noindent On the plot below, show one gradient step (with an arrow) for each of the methods
mentioned below. The minimum is the black circle, whereas your position is the black 
square. Make sure you label the three arrows in your answer. 
\begin{enumerate}
  \item Standard gradient
  \item Natural gradient (or Newton’s method)
  \item Adagrad or RMSprop (assume they have run for a while to accumulate gradient
  information)
\end{enumerate}

\noindent Explain the direction of the arrow you designed. \\ \\

\noindent \underline{\textbf{\textit{Solution:}}} \\ 

\begin{enumerate}
  \item The plot after one gradient step for the Standard gradient method:
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.5\textwidth]{pr9_a.png}
          
        \end{figure}
        
  \item The plot after one gradient step for the Natural gradient method:
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.5\textwidth]{pr9_b.png}
          
        \end{figure}
  \item The plot after one gradient step for the Adagrad or RMSprop method:
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.5\textwidth]{pr9_c.png}
          
        \end{figure}
\end{enumerate}
\noindent All the methods take a step in the direction of the negative gradient 
of the function at the current point. The step for the Natural gradient method is larger than the steps of the other methods. Adagrad/RMSprop method has the smallest step.




%Problem 10
\newpage
\noindent \textbf{Problem 10} \\

\noindent You are given the description of the following optimization method, where $ \alpha, \beta, \nu \in R, (\alpha $
is the learning rate):

\begin{center}
$g_{t+1} \leftarrow \beta \cdot g_t + (1 - \beta) \cdot \nabla L_t(\theta_t) $
\end{center}
\begin{center}
$\theta_{t+1} \leftarrow \theta_t - \alpha \left[ (1 - \nu) \cdot \nabla L_t(\theta_t) + \nu \cdot g_{t+1} \right] $
\end{center}

\noindent For which values of $ \beta $ and $\nu$ you get optimization methods familiar to you (described in
our lectures)? Name them, and write the respective equations.\\ \\ \\

\noindent \underline{\textbf{\textit{Solution:}}}

\noindent The above equations can represent 2 different optimization methods:

\begin{enumerate}
  \item For $\beta = 0$ and $\nu = 1$  we get the standard gradient descent optimization method
    \begin{center}
    $g_{t+1} \leftarrow \nabla L_t(\theta_t) $
    \end{center}
    \begin{center}
    $\theta_{t+1} \leftarrow \theta_t - \alpha \cdot g_{t+1}$
    \end{center}
    
  \item For $\beta \neq 0$ and $\nu = 1$ we get the SGD with monmentum optimization method
    \begin{center}
    $g_{t+1} \leftarrow \beta \cdot g_t + (1 - \beta) \cdot \nabla L_t(\theta_t) $
    \end{center}
    \begin{center}
      $\theta_{t+1} \leftarrow \theta_t - \alpha \cdot g_{t+1} $
    \end{center}
  \end{enumerate}
  







%Problem 11
\newpage
\noindent \textbf{Problem 11}

\noindent Consider the following input image: \\ \\
\begin{center}  

 $ I = \begin{bmatrix}
    20 & 35 & 35 & 35 &35 & 20 \\
    29 & 46 & 44 & 42 &42 & 27 \\
    16 & 25 & 21 & 19 &19 & 12 \\
    66 & 120 & 116 & 154 &114 & 62 \\
    74 & 216 & 174 & 252 &172 & 112 \\
    70 & 210 & 170 & 250 &170 & 110
  \end{bmatrix}$ 
\end{center}

\vspace{0.5cm}
\begin{enumerate} [label=\Alph*]
    
    \item  What is the output provided by a convolution layer with the following properties: \\ \\
        stride=(1, 1), and kernel = $\begin{bmatrix}
    1 & 1 & 1 \\
    1 & 0 & 1  \\
    1 & 1 & 1 \\
    
    \end{bmatrix}$
    \item Take the output from A. and apply a max pooling layer with the following properties: \\ \\
        stride=(2,2), window shape=(2, 2).
    \item For the following kernels, describe what kind of feature they extract from the image:
    \begin{center}  
         $ F1 = \begin{bmatrix}
            -10 & -10 & -10 \\
            5 & 5 & 5  \\
            -10 & -10 & -10 \\
          \end{bmatrix}$ 
\end{center}
\begin{center}  
         $ F2 = \begin{bmatrix}
            2 & 2 & 2 \\
            2 & -12 & 2  \\
            2 & 2 & 2 \\
          \end{bmatrix}$ 
\end{center}
\begin{center}
    $ F3 = \begin{bmatrix}
        -20 & -10 & 0 & 5 &10 \\
        -10 & 0 & 5 & 10 &5 \\
        0 & 5 & 10 & 5 &0 \\
        5 & 10 & 5 & 0 &-10 \\
        10 & 5 & 0 & -10 &-20 \\
    \end{bmatrix}$ 
\end{center}

    
\end{enumerate}

\noindent \underline{\textbf{\textit{Solution:}}} \\ 

\begin{enumerate} [label=\Alph*]
    \item In order to get the output of the convolution layer with stride = (1,1), we need to follow the steps described below: \\
          \begin{figure}[h]
            \centering
            \includegraphics[width=0.5\textwidth]{prob11_a.png} \\      
          \end{figure}
          \newpage
          \noindent The output will be a 4x4 table where the first element's value (out[1,1]) is calculated by multiplying each element of kernel with the corresponding 
          element of the red subtable and then adding them all together. Knowing that the given stride is (1,1) we will then repeat the same process using the blue subtable in 
          order to calculate out[1,2]. After completing the same process for the first row of I, we will then proceed to calculate the values of out in the second row. We will follow 
          the same process described above, using the green subtable for out[2,1] and so on. \\ \\ 
          So $out[1,1] = 1*20 + 1*35 + 1*35 + 1*29 + 0*46 + 1*44 + 1*16 + 1*25 + 1*21 \Rightarrow \bm{out[1,1] = 225}$\\ \\
          The rest of the element are calculated likewise. The output is:
        \begin{center}
        $ out = \begin{bmatrix}
            225 & 258 & 250 & 209 \\
            458 & 566 & 552 & 372 \\
            708 & 981 & 887 & 802 \\
            1000 & 1488 & 1320 & 1224 
        \end{bmatrix}$ \\
        \end{center}
        
    
    \item  Applying a max pooling layer with stride = (2,2) and window shape = (2,2) results in a 2x2 max-pooled output, where the first element is equal to the element of maximum value in the first subtable shown below and so on. 
        \begin{figure}[h]
          \centering
          \includegraphics[width=0.4\textwidth]{prob11_b.png} \\      
        \end{figure}
        \newline So the output after the max pooling layer is:
        \begin{center}
        $ max-pooled = \begin{bmatrix}
            566 & 552 \\
            1488 & 1320
        \end{bmatrix}$ 
        \end{center}
    
    \item In the image below we can see the result of applying each kernel to the input image: \\
        \begin{center}
            \includegraphics[width=0.8\textwidth]{problem11images.png}
        \end{center}
        Features extracted from the image:
        \begin{itemize}
          \item F1 kernel appears to detect horizontal edges
          \item F2 kernel detects edges in all directions
          \item F3 kernel detects edges on a diagonal.
        \end{itemize}
        
        
            
\end{enumerate}\



%Problem 12
\vspace{2cm}
\noindent \textbf{Problem 12}

\noindent Consider a 1D convolutional network where the input has three channels. The first 
hidden layer is computed using a kernel size of three and has four channels. The second
hidden layer is computed using a kernel size of five and has ten channels. How many 
biases and how many weights are needed for each of these two convolutional layers? \\ \\ \\

\noindent \underline{\textbf{\textit{Solution:}}} \\ 

\begin{itemize}
  \item  For the first hidden layer we have 3 input channels and a kernel of size 3 with 4 channels. \\So $3*3^2*4=108$ weights and 4 biases (one for each filter channel).
  \item  For the second hidden layer we have 4 input channels and a kernel of size 5 with 10 chennels. So $4*5^2*10=1000$ weigths and 10 biases.
\end{itemize}
In total we need $1000+108=\bm{1108}$ \textbf{weights} and $4+10=\bm{14}$ \textbf{biases}.  



%Problem 13
\newpage
\noindent \textbf{Problem 13}

\noindent Max-pooling can be accomplished using ReLU operations.
\begin{enumerate} [label=\Alph*]
    
    \item Express max(a, b) by using only ReLU operations.
    \item Use this to implement max-pooling by means of convolutions and ReLU layers.
    \item How many channels and layers do you need for a 2x2 convolution? How many for a 3x3 convolution?\\ \\ \\
\end{enumerate}\

\noindent \underline{\textbf{\textit{Solution:}}}
\begin{enumerate} [label=\Alph*]
    
        \item  Knowing that
            \[ ReLU = \begin{cases}
                x & \text{for } x > 0 \\
                0 & \text{for } x \leq 0 \\
            \end{cases} \] \\ \\If $a > b$ then $a - b > 0$ and $ReLU(a-b) = a - b$. So, $ReLU(a-b) + b = a - b + b = a$ \\ \\
          If $a \leq b$ then $a - b \leq 0$ and $ReLU(a-b) = 0$. So, $ReLU(a-b) + b = 0 + b = b$ \\ \\ \\
          Using only ReLU operations max(a,b) is expressed: $\bm{max(a,b) = ReLU(a-b) +b}$ \\

          \item Let us use an input matrix I and a 2x1 convolution filter F: \\ \\
          \[
            I = \begin{bmatrix}
                1 & 8 & 15 & 16 \\
                5 & 6 & 18 & 20 \\
                7 & 3 & 11 & 17 \\
                2 & 1 & 8 & 16
            \end{bmatrix}
            \quad
            \hspace{1cm}
            F = \begin{bmatrix}
                1 \\ -1
            \end{bmatrix} 
            \]
          
          \vspace{0.8cm}  
          The output of the convolution with stride (1,1) is:  \\\\
          $ conv = 
          \begin{bmatrix}
            1-5 & 8-6 & 15-18 & 16-20 \\
            5-7 & 6-3 & 18-11 & 20-17 \\
            7-2 & 3-1 & 11-8 & 17-16
          \end{bmatrix} \Rightarrow conv=
          \begin{bmatrix}
              -4 & 2 & -3 & -4 \\
              -2 & 3 & 7 & 3 \\
              5 & 2 & 3 & 1
          \end{bmatrix}$
          \vspace{1cm} 

          Using a ReLU operation on the convolution, the output is: \\ \\
          $ conv(ReLU) = \begin{bmatrix}
              0 & 2 & 0 & 0 \\
              0 & 3 & 7 & 3 \\
              5 & 2 & 3 & 1
          \end{bmatrix}$, where every negative element of the conv matrix is turned to 0. 

          \vspace{0.8cm}
          In order to get the max-pooling output we need to add the output of the ReLU operation to the 3x4 matrix (starting from the second row of the initial input matrix I):\\ \\

          $max-pooled = \begin{bmatrix}
            0 & 2 & 0 & 0 \\
            0 & 3 & 7 & 3 \\
            5 & 2 & 3 & 1
          \end{bmatrix} + 
          \begin{bmatrix}
                5 & 6 & 18 & 20 \\
                7 & 3 & 11 & 17 \\
                2 & 1 & 8 & 16
          \end{bmatrix} \Rightarrow max-pooled = \begin{bmatrix}
              5 & 8 & 18 & 20 \\
              7 & 6 & 18 & 20 \\
              7 & 3 & 11 & 17
          \end{bmatrix}$

          \item 
          For both the 2x2 and 3x3 convolutions, the number of channels needed depends on the number of input channels. In the examples given below we have only 1 input channel.\\\\
          For a 2x2 convolution let's use an input matrix I and filter F: \\
          \[
            I = \begin{bmatrix}
                1 & 8\\
                5 & 6 \\
            \end{bmatrix}
            \quad
            \hspace{1cm}
            F = \begin{bmatrix}
                1 \\ -1
            \end{bmatrix} 
            \]

            By applying F to the 2 columns of I, we get $1-5=-4$ and $8-6=2$. \\So, out = 
            $\begin{bmatrix}
              -4 &2 \\
            \end{bmatrix} $
            \\ \\By applying a ReLU layer we get: outReLu = $
            \begin{bmatrix}
              0&2
            \end{bmatrix}$.
            \\ Adding the second row elements of I to outReLU, we get $Result = \begin{bmatrix}
              5 & 8
            \end{bmatrix}$, which are the max values of each column of I. \\ \\
            In order to get the max value of Result we use another convolution layer with filter:\\
            $ F2 = \begin{bmatrix}
              1 & -1
            \end{bmatrix}$. By applying F2 we get $5-8=-3$. Using a ReLU operation and adding the second element of Result we get the max value which is 8.\\\\
            So, in total we have 2 convolution layers and 2 ReLU layers.\\ \\\\For a 3x3 convolution let's use an input matrix I and filter F:
            \[
              I = \begin{bmatrix}
                  1 & 8 & 15\\
                  5 & 6 & 18\\
                  7 & 3 & 11
              \end{bmatrix}
              \quad
              \hspace{1cm}
              F = \begin{bmatrix}
                  1 \\ -1
              \end{bmatrix} 
              \]

            By applying F we get: out=$ 
            \begin{bmatrix}
              1-5& 8-6 & 15-18 \\
              5-7& 6-3 &18-11             
            \end{bmatrix} =\begin{bmatrix}
              -4 & 2 & -3\\
              -2 & 3 & 7
            \end{bmatrix}$ 
            \\ \\ Then we apply a ReLU operation to each result: \\ \\
            outReLU = $
            \begin{bmatrix}
              0 & 2 & 0\\
              0 & 3 & 7
            \end{bmatrix}$. \\\\Adding the last 2 rows of I to outReLU we get:
             
            $Result = \begin{bmatrix}
              5 & 8 & 18 \\
              7 & 6 & 18
            \end{bmatrix}$ 

            We then use another convolution layer using filter
            $F = \begin{bmatrix}
              1 & -1
            \end{bmatrix}$.
            So, we get $out = \begin{bmatrix}
                -3 & -10 \\
                1 & -12
              \end{bmatrix}$. \\ \\ After a ReLU operation we get outReLU =$\begin{bmatrix}
                0& 0 \\
                1 & 0
              \end{bmatrix}$\\ \\Adding the last 2 columns of Result to outReLU we get $Result' =\begin{bmatrix}
                8 & 18 \\
                7 & 18
              \end{bmatrix}$
            \\\\ By applying F and a ReLU operation we get:
            $out = \begin{bmatrix}
              18 & 18
            \end{bmatrix}$ 
            \\\\ The final step is to apply another convolution layer using F. Repeating the same process, we get the max value which is 18.
            
           So, in total we have 4 convolution layers and 4 ReLU layers.

             


          

          
\end{enumerate}\

%Problem 14
\newpage
\noindent \textbf{Problem 14}

\noindent Consider a convolutional neural network that is used to classify images into two classes.
The structure of the network is as follows:

\begin{itemize}
    \item INPUT: 100x100 grayscale images.
    \item LAYER 1: Convolutional layer with 100 5x5 convolutional lters.
    \item LAYER 2: Convolutional layer with 100 5x5 convolutional lters.
    \item LAYER 3: A max pooling layer that down-samples LAYER 2 by a factor of 4 (from 100x100 $\rightarrow$ 50x50)
    \item LAYER 4: Dense layer with 100 units
    \item LAYER 5: Dense layer with 100 units
    \item LAYER 6: Single output unit
\end{itemize}

\noindent How many weights does this network have? \\

\noindent \underline{\textbf{\textit{Solution:}}} 

\begin{itemize}
  \item For the first layer we have $5*5 = 25$ weights per filter. We also have 100 filters so $25*100=2500$ total weights and 100 biases.
  \item For the second layer we have $5*5=25$ weights per filter. We also have 100 input channels and 100 filters for each channel so $25*100*100=250000$ total weights and 100 biases.
  \item For the third layer no weights are required as it is a max pooling layer.
  \item For the fourth layer we have 100 input channels and an image size of 50x50 so $50*50*100=250000$ total inputs.
        Because this is a dense layer each of them must connect to all 100 ouptut units. So we have $250000*100=25000000$ total weigths and 100 biases (one for each output unit).
  \item For the fifth layer we have a dense layer where each of our 100 input units must connect with all 100 output units. So $100*100=10000$ total weights and 100 biases.
  \item For the last layer our 100 input units must connect to the single output unit so we have 100 weights and one bias. 
\end{itemize}

\noindent Adding the weights each layer has we have $2500+250000+25000000+10000+100 = \\\bm{2762600}$ \textbf{weights} and \textbf{401 biases}.

%Problem 15
\newpage
\noindent \textbf{Problem 15}

Your task is to implement fast convolutions with a kxk kernel. One of the algorithm
candidates is to scan horizontally across the source, reading a k-wide strip and computing
the 1-wide output strip one value at a time. The alternative is to read a k + $\Delta$ wide strip
and compute a $\Delta$-wide output strip.
\begin{enumerate} [label=\Alph*]
    \item Why is the latter preferable? Is there a limit to how large you should choose $\Delta$?
    \item Create a sample image of 228x228 and a series of 3x3, 7x7 and 11x11 kernels, and
measure the execution time of the two alternatives. Comment on your results.

\end{enumerate}
\vspace{1cm}

\noindent \underline{\textbf{\textit{Solution:}}}


\noindent 
\begin{enumerate} [label = \Alph*]
  \item The latter is preferable for a number of reasons:
  \begin{itemize}
    \item By reading larger strips the alogrithm needs to access the memory less times, speeding up the process.
    \item By computing a $\Delta$-wide output strip we may be able to compute the output in parallel using more cpu cores.
    \item Accessing continuous blocks of data improves cache efficiency.
  \end{itemize}
  However there is a limit to how large $\Delta$ can be. A large $\Delta$ requires a lot of memory and may also require computing overhead to manage all the opperations.
  This is why it is important to find a balance.

  \item For the experiment we have created and run both alogrithms. The results can be seen in the graphs below.
  We run the first algorithm 20 times and ploted the average. We also run the second algorithm 20 times for each delta and plotted the average:

  \begin{itemize}
    \item For the kernel of size 3x3 the graph shows the optimal $\Delta$ value is 2: \\ \\
    \includegraphics[width=0.8\textwidth]{Problem15_3.png} \newpage
    \item For the kernel of size 7x7 the graph shows the optimal $\Delta$ value is 5: \\ \\
    \includegraphics[width=0.8\textwidth]{Problem15_7.png}\newline \\
    \item For the kernel of size 11x11 the graph shows the optimal $\Delta$ value is 8: \\ \\
    \includegraphics[width=0.8\textwidth]{Problem15_11.png}
  \end{itemize}
  \noindent \\ We observe that there is a different optimal $\Delta$ value for each kernel size and that after a certain point 
  increasing the $\Delta$ further produces worse results.
  
\end{enumerate}


\end{document}




