\documentclass{article}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{float}
\usepackage{accents}
\usepackage{undertilde}

% added this to stop hbadness warning
\hbadness = 5000000

\geometry
{
	headheight = 4ex,
	includehead,
	includefoot,
	paper = a4paper,
	inner = 2.5cm,
	outer = 2.5cm,
	bindingoffset = 0.5cm,
	top = 2cm,
	bottom = 1.5cm
}


\title{\Huge Problem Set 3} 
\vspace{1cm}
\author {\Large Chara Tsirka 03315, Prodromos Avramidis 03291}

\begin{document}

\maketitle
\begin{center}
\vspace{1cm}
\includegraphics[width=0.3\textwidth]{uthlogo.png}
\vspace{2cm}
\end{center}
\begin{center}
  \Huge Neurofuzzy Computing \vspace{1cm}

  \Large Fall Semester 2023-2024 \vspace{1cm}

  \Large Professor: Dimitrios Katsaros
\end{center}


%Problem 1
\newpage
\noindent \textbf{Problem 1}

\noindent Design by hand (i.e., select manually the layers, the number of neurons per layer, the 
weights and biases per layers) an RBF network to perform the classification illustrated in 
the following figure. The network should produce a positive output whenever the input 
vector is in the shaded region and a negative output otherwise. Provide arguments for the 
selection of weights and biases
\begin{center}    
    \includegraphics[width=0.4\textwidth]{pr1.png}
    \vspace{2cm}
\end{center}



\noindent \underline{\textbf{\textit{Solution:}}}

\noindent From the problem statement we know that the RBF network will have two inputs and one output.
We will use two neurons in the first layer because we have 2 shaded regions. 
The rows of the first layer weight matrix
will create the centers for the two basis functions so that we can produce the maximum outpus there. 
As we can see in the figure the center for the first shaded region
is (-1, 1.5) and the center for the second shaded region is (2,2) so the first layer weights matrix will be: \\
\begin{center}
  
$ W^1 = \begin{bmatrix}
  - 1 & 1.5 \\
  2 & 2
\end{bmatrix} $
\end{center}

\noindent Now we need to choose a biases for the first layer. We see in the figure that the radius of the first 
region is approximately 0.5 and the radius of the second region is approximately 0.25. 
Because the first region is wider than the second region the first bias will be smaller than the second bias.
If we choose biases 2 and 4 respectively we get:
$a = e^{-n^2}= e^{-(1*0.5)^2} \approx 0.18 and a = e^{-n^2}= e^{-(2*0.25)^2} \approx 0.18$
Therefore, each basis function will have a peak of 1 at the centers, and will drop to 0.18 at the edge of the regions
So the first layer bias will be:
\begin{center}
  
  $ b^1 = \begin{bmatrix}
    1 \\
    2 
  \end{bmatrix} $
  \end{center}

  \noindent We want the output to be negative for inputs outside the shaded regions,
  so we will use a bias of -1 for the second layer, and we will use a value of 2
  for the second layer weights, in order to bring the peaks back up to 1. The second layer weights and bias are:

  \begin{center}
  
    $ W^2 = \begin{bmatrix}
      2 & 2
    \end{bmatrix},
     $
    $ b^2 = \begin{bmatrix}
      -1
    \end{bmatrix} $

    \end{center}
  
  



%Problem 2
\newpage
\noindent \textbf{Problem 2}

\noindent Write a MATLAB/python/â€¦ program to implement the steepest descent algorithm for the $ 1-S^1-1$ RBF network. 
Train the network to approximate the function:
\begin{center}
    $g(p) = 1 +sin(p*\pi/8) for -4 \leq p \leq 4$.    
\end{center}  

\begin{itemize}
    \item To train the network, select 30 data points at random from the interval $-4 \leq p \leq 4$.
    \item Initialize all parameters (weights and biases in both layers) as small random numbers, 
    and then train the network to convergence. (Experiment with the learning rate $\alpha$, to 
    determine a stable value.) Plot the network response for $-4 \leq p \leq 4$, and show the 
    training points on the same plot. Compute the sum squared error over the training 
    set. Use 4, 8, 12 and 20 centers. Try different sets of initial weights, different 
    sampling methods for selecting training pairs and record your observations. \\ \\
\end{itemize}

\noindent \underline{\textbf{\textit{Solution:}}}

\noindent 
We run the program with a learning rate of a = 0.1 with random reference points and with uniformly distributed reference points:

\begin{itemize}
	\item Random reference points: \\
	The sum squared errors for 4,8,12 and 20 centers are 1.617, 0.22116, 0.28262 and 0.29801 respectively. The plots are: \\
	\includegraphics[width=0.4\textwidth]{Problem2_4_random.png}
	\includegraphics[width=0.4\textwidth]{Problem2_8_random.png} \\
	\includegraphics[width=0.4\textwidth]{Problem2_12_random.png}
	\includegraphics[width=0.4\textwidth]{Problem2_20_random.png}

	\item Uniform reference points: 
	The sum squared errors for 4,8,12 and 20 centers are 0.48242, 0.048453, 0.17237 and 0.044526 respectively. The plots are: \\
	\includegraphics[width=0.4\textwidth]{Problem2_4_uniform.png}
	\includegraphics[width=0.4\textwidth]{Problem2_8_uniform.png} \\
	\includegraphics[width=0.4\textwidth]{Problem2_12_uniform.png}
	\includegraphics[width=0.4\textwidth]{Problem2_20_uniform.png}
\end{itemize}

We can observe from the errors and the plots that the uniform reference points produce much better results.


%Problem 3
\newpage
\noindent \textbf{Problem 3}

\noindent The net input expression for LVQ networks calculates the distance between the input 
and each weight vector directly, instead of using the inner product. The result is that the 
LVQ network does not require normalized input vectors. This technique can also be 
used to allow a competitive layer to classify non-normalized vectors. Such a network is 
shown in figure below.

\begin{center}
    \includegraphics[width=0.4\textwidth]{pr3.png}
\end{center}

\noindent Use this technique to train a two-neuron competitive layer on the (non-normalized) 
vectors below, using a learning rate, $\alpha$, of 0.5 

\begin{center}
    $p_1 = \begin{bmatrix}
        1\\
        1
      \end{bmatrix}, p_2 = \begin{bmatrix}
        -1\\
        2
      \end{bmatrix}, p_3 = \begin{bmatrix}
        -2 \\
        2
      \end{bmatrix}$
\end{center}

\noindent Present the vectors in the following order: $p_1, p_2, p_3, p_2, p_3, p_1$. 
Here are the initial weights of the network: 

\begin{center}
    $w_1 = \begin{bmatrix}
        0\\
        1
      \end{bmatrix}, w_2 = \begin{bmatrix}
        1\\
        0
      \end{bmatrix}$
      \vspace{1cm}
\end{center}

\noindent \underline{\textbf{\textit{Solution:}}}

\noindent Presenting the vectors in the given order we get: \\\\ \underline{$p_1$:}


%Problem 4
\newpage
\noindent \textbf{Problem 4}

\noindent Repeat Problem-03 for the following inputs and initial weights. Show the movements of 
the weights graphically for the first six each steps. If the network is trained for a large 
number of iterations, how will the three vectors be clustered in the final configuration?

\begin{center}
    $p_1 = \begin{bmatrix}
        2\\
        0
      \end{bmatrix}, p_2 = \begin{bmatrix}
        0\\
        1
      \end{bmatrix}, p_3 = \begin{bmatrix}
        2\\
        2
      \end{bmatrix}$\\
      \vspace{1cm}
      $w_1 = \begin{bmatrix}
        1\\
        0
      \end{bmatrix}, w_2 = \begin{bmatrix}
        -1\\
        0
      \end{bmatrix}$
      \vspace{1cm}

\end{center}

\noindent \underline{\textbf{\textit{Solution:}}}


%Problem 5
\newpage
\noindent \textbf{Problem 5}

\noindent You are asked to generate an Auto Regressive (AR) model and then create an RNN (such 
as LSTM, or GRU) that predicts it. Generate samples of an Auto Regressive model of 
the form: 
\begin{center}
    $X_t = a_1X_{t-1} + a_2X_{t-2} + a_3X_{t-3} + U_t$
    
\end{center}
where $a_1= 0.5, a_2= -0.1, a_3= 0.2$ and $U_t$ is independent-identically distributed Uniform in 
the interval (-0.25, 0.25). Now train an RNN of your choice that predicts the sequence. 

\begin{enumerate} [label=\Alph*]
    \item Apply the training algorithm on new samples and calculate the averaged cost square 
    error cost function. Investigate different number of training samples. (You are not 
    allowed to use your model knowledge when you design the RNN.)
    \item Optional part: You may use the Yule-Walker equations to compare your RNN 
    model. 
    
\end{enumerate}

\vspace{1cm}

\noindent \underline{\textbf{\textit{Solution:}}}


%Problem 6
\newpage
\noindent \textbf{Problem 6}

\noindent Consider the following Moving Average (MA) process:
\begin{center}
    $X_t = U_t + a_1U_{t-1} + a_2U_{t-2} + a_3U_{t-3} + a_4U_{t-4}+ a_5U_{t-5}+ a_6U_{t-6}$
\end{center}
\noindent where $a_1= a_2=5, a_3=a_4=a_5=a_6= -0.5$ and $U_t$ independent-identically distributed Uniform \\
(0, 0.5). 
Repeat the exercise above with a moving average model. \\ \\

\noindent \underline{\textbf{\textit{Solution:}}}



%Problem 7
\newpage
\noindent \textbf{Problem 7}

\noindent Model the following expressions as fuzzy subsets:
\begin{enumerate} [label = \Alph*]
    \item Large integers.
    \item Very small numbers.
    \item Medium-weight men.
    \item Numbers approximately between 10 and 20.
\end{enumerate}

\vspace{1cm}

\noindent \underline{\textbf{\textit{Solution:}}}


%Problem 8
\newpage
\noindent \textbf{Problem 8}

\noindent During the lectures, we have defined the concept of ordinary subset of level $\alpha$ of a fuzzy 
subset. The analogous definition, i.e., the ordinary relation of level $\alpha$ of a fuzzy relation, is 
straightforward. Determine analytically and show graphically the ordinary relation of level 
0.3 for the fuzzy relation with membership function: $\mu_{\utilde{R}}(x, y) = 1 - 1/(1+x^2+y^2)$.
\\ \\
\noindent \underline{\textbf{\textit{Solution:}}}

\noindent Analytically we need to solve: \\
$1-1/(1+x^2+y^2) \geq 0.3 $ \\ $\rightarrow 1+x^2+y^2 \geq 1/0.7 $ \\
$ \rightarrow x^2 + y^2 \geq 3/7$
\\ \\
\noindent Graphically we get the following plot:

\begin{center}
\includegraphics[width=0.6\textwidth]{Problem8.png}
\end{center}





%Problem 9
\newpage
\noindent \textbf{Problem 9}

\noindent Consider the reference set E= $[0, \alpha] \subset R$. If $\utilde{A}$ is the fuzzy subset defined by 
$\mu_{\utilde{A}}(x)$, give the index v of fuzziness of $\utilde{A}$ for:

\begin{enumerate} [label=\Alph*]
  \item $\mu_{\utilde{A}}(x)$ = $x^2/\alpha^2$, $x \in [0, \alpha].$
  \item $\mu_{\utilde{A}}(x) = 4x^2/\alpha^2$ if $0 \leq x\leq \alpha/2$ and $\mu_{\utilde{A}}(x) = 4(x-\alpha)^2/\alpha^2$ if $\alpha/2  <x \leq \alpha$
\end{enumerate}

\noindent \underline{\textbf{\textit{Solution:}}} \\

\noindent In order to calculate the index of fuzziness we will use the formula: $\nu = \frac{2}{n} * d(\utilde A,  \bar{ \utilde{ A}})$


\noindent First we will calculate the distance: \\ 
$d(\utilde A,\utilde B) = \sum_{i=1}^{n} | \mu_A(x_i) -  \mu_B(x_i)|$
  $\rightarrow d(\utilde A,\bar{ \utilde{ A}} ) = \sum_{i=1}^{n} | \mu_A(x_i) -  \mu_{\bar{ \utilde{ A}}}(x_i)|$ 
  and $\mu_{\bar{ \utilde{ A}}}(x_i) = 1 - \mu_A(x_i)$ \\
  So $ d(\utilde A,\bar{ \utilde{ A}} ) = \sum_{i=1}^{n} | 2* \mu_A(x_i) -  1| $ \\
\begin{enumerate} [label = \Alph*]
\item
  For the given membership function we get:\\ \\
  $ d(\utilde A,\bar{ \utilde{ A}} ) = \sum_{0}^{a} | 2* \frac{x^2}{a^2} -  1| = 2 $ \\
  So $\nu = \frac{2}{\alpha}* d(\utilde A,\bar{ \utilde{ A}} ) = \frac{4}{a}$

  \item For the given membership functions we get:\\ \\
  $ d1(\utilde A,\bar{ \utilde{ A}} ) = \sum_{0}^{a/2} | 2* \frac{4*x^2}{a^2} -  1| = 8 $ \\
  $ d2(\utilde A,\bar{ \utilde{ A}} ) = \sum_{a/2}^{a} | 2* \frac{4*(x-a)^2}{a^2} -  1| = -2 $ \\
  $ d(\utilde A,\bar{ \utilde{ A}} ) = d1 + d2 = 6 $
  So $\nu = \frac{2}{\alpha}* d(\utilde A,\bar{ \utilde{ A}} ) = \frac{12}{a}$

\end{enumerate}

\end{document}